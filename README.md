# house-price-prediction
This repository is about processing scraped house/apartment data and creating a price prediction model.

Business Need:
An initiative to transform Eurowings into a booking platform, through a website Eurowings will offer hotels and apartments and want to build price prediction 
model for the same from the historical data.


Current Solution Strategy:
The whole solution is divided into two parts.
i) Pre-processing the booking data/historical data through pyspark pre-processor and store it on a distributed file system.
ii)Ingest acquired data to the Multiple Linear Regression model to predict the price based on the supplied features.


Part-i) Implementation for HousePricingPreProcessor.py (Data Engineering part)

- Create a Spark Application which reads the dataset from the source system.

- The dataset files contain broken records or multi-line records and many fields are given in double quotes (""), so we need to supply this configuration
  while reading the csv file such that it will not treat the in-between commas in double quotes as separate fields.

- The dataset contains total 96 fields, not all of them are required for deriving the price prediction model. Therefore, we need to eliminate some fields 
  like summary, space, descriptions, neighbors, notes, urls, etc.... These fields are descriptive and they don't add any values in our prediction model.

- Assumption : After having a discussion with data scientists we decided to capture below set of fields only.
	{'last_scraped','city','state','latitude','longitude','property_type','room_type','accommodates','bathrooms','bedrooms','beds','guests_included','review_scores_rating','price'}

- We will perform below set of data pre-processing, cleaning or manipulation action.
	-> Remove all the empty rows from derived dataframe.
	-> Remove special characters like '$' and ',' from price column or related columns containing the same format.
	-> It may happen that people might have not given review for some houses so, Replace NaN or null values with '0' for column['review_scores_rating']
	-> column['last_scraped'] contains TimeStamp so apply substring method on this column and fetch only date part as this column may be useful while creating 
	   a partition while writing data onto distributed system (HDFS)
	
- Write the data onto the HDFS in desired CSV format as per the file output path given (partition can be done on "last_scraped" column).

- Optionally, We can create a hive table on the acquired data and keep appending the data as it arrives like daily, weekly or monthly.



Part-ii) Implementation for HousePricePredictor.py (Data Scientist part - Use the processed data to derive price prediction model)

- Create a Machine Learning Model using Pandas and sci-kit learn libraries or use Spark with Mlib (completely depends on data scientists choice and as per 
  the requirement), For an easy way I have selected pandas and sci-kit learn to create a price prediction model.

- With the help of pandas library read the processed data generated by part-i and choose specific features.

- We will perform below set of action for missing values.
	-> Reject records where city = null or blanks.
	-> For some of the rows the values for bathrooms are 'NaN' so we will fill these cells with value '1' (Assumption, A house can have minimum 1 bathroom)
	-> For few rows the values for bedrooms are 'NaN' so we will put value '1' for these cells (Assumption, A house can have minimum 1 bedroom)
	-> For some of the rows the values for beds are 'NaN' so we will put value '2' for these cells (Assumption, A house can have 2 or more beds, so minimum 2)

- The above missimg values can be filled up by taking min, max or average of a column or it can be derived from other dependent columns, for simplycity I have hardcoded them with some integer values. 

- From data frame select below metrics of features X and price as y
	-> X = {'latitude','longitude','room_type','accommodates','bathrooms','bedrooms','beds','review_scores_rating'}
	-> y = {'price'}
	
- In above metrics X, the column 'room_type' contains categorical data so we will perform Encoding for that using LabelEncoder followed by OneHotEncoder.

- Once Encoding is done, we will split our whole data set into the training set and the test set with test size = 0.2

- We will apply the Linear Regression Model from sci-kit learn library onto the Training data set. and once our model is trained and ready we will derive/predict 
  the price for the test data we have.

- Finally, to check/validate our Price Prediction Model we will calculate R2-score so that we can make sure whether our model is performing good or not.



---------------------------------------------------------------------------------------------------------------------------------------------
Additional Functionalities:

1) For continuous update of the data we can execute the part-1 on daily, weekly or monthly bases as per the file/data arrival.
	-> Meaning we can have an automatic trigger job which keeps monitoring the source file  location and will trigger the spark job once the file is available.

2) The hive table can be created based on the date partition column.

3) The data scientists team can use the data present in the acquired hive table and can train the model as per their need or whenever new data is arrived.

4) To deal terabytes of data, we can provision the capacity of our hadoop cluster in such a way that it can easily execute the pre-processing job (part-i) and
   Prediction job using Machine Learning model(part-ii).
   -> If organization is planning to go onto the cloud then we can use AWS services like Spark on EMR and SageMaker. The same code can be executed from cloud. 
   And AWS provides Auto Scaling features for many of its services like Lambda, DynamoDB, EMR, etc..

